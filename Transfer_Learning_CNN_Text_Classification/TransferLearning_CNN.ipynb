{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Transfer Learning with CNN on the classic IMDB Sentiment</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Import Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import gc\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "#nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english')) \n",
    "\n",
    "import gensim\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data_utils\n",
    "\n",
    "\n",
    "from torchtext import data\n",
    "from torchtext import datasets\n",
    "from torchtext.vocab import Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Data and Prepare it for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data/imdb_master.csv', encoding=\"latin-1\")\n",
    "df = df.drop(['Unnamed: 0', 'file'], axis=1)\n",
    "\n",
    "df = df[df['label'] != 'unsup']\n",
    "df['label'] = df['label'].replace('neg', 0)\n",
    "df['label'] = df['label'].replace('pos', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['label'] = df['label'].replace('neg', 0)\n",
    "df['label'] = df['label'].replace('pos', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Contractions Dictionary\n",
    "contractions = { \n",
    "\"ain't\": \"am not\",\n",
    "\"aren't\": \"are not\",\n",
    "\"can't\": \"cannot\",\n",
    "\"can't've\": \"cannot have\",\n",
    "\"'cause\": \"because\",\n",
    "\"could've\": \"could have\",\n",
    "\"couldn't\": \"could not\",\n",
    "\"couldn't've\": \"could not have\",\n",
    "\"didn't\": \"did not\",\n",
    "\"doesn't\": \"does not\",\n",
    "\"don't\": \"do not\",\n",
    "\"hadn't\": \"had not\",\n",
    "\"hadn't've\": \"had not have\",\n",
    "\"hasn't\": \"has not\",\n",
    "\"haven't\": \"have not\",\n",
    "\"he'd\": \"he had\",\n",
    "\"he'd've\": \"he would have\",\n",
    "\"he'll\": \"he will\",\n",
    "\"he'll've\": \"he will have\",\n",
    "\"he's\": \" he is\",\n",
    "\"how'd\": \"how did\",\n",
    "\"how'd'y\": \"how do you\",\n",
    "\"how'll\": \"how will\",\n",
    "\"how's\": \"how is\",\n",
    "\"I'd\": \"I had\",\n",
    "\"I'd've\": \"I would have\",\n",
    "\"I'll\": \"I will\",\n",
    "\"i'll\": \"I will\",\n",
    "\"I'll've\": \"I will have\",\n",
    "\"I'm\": \"I am\",\n",
    "\"I've\": \"I have\",\n",
    "\"isn't\": \"is not\",\n",
    "\"it'd\": \"it would\",\n",
    "\"it'd've\": \"it would have\",\n",
    "\"it'll\": \"it will\",\n",
    "\"it'll've\": \"it will have\",\n",
    "\"it's\": \"it is\",\n",
    "\"it's\": \"it is\",\n",
    "\"let's\": \"let us\",\n",
    "\"ma'am\": \"madam\",\n",
    "\"mayn't\": \"may not\",\n",
    "\"might've\": \"might have\",\n",
    "\"mightn't\": \"might not\",\n",
    "\"mightn't've\": \"might not have\",\n",
    "\"must've\": \"must have\",\n",
    "\"mustn't\": \"must not\",\n",
    "\"mustn't've\": \"must not have\",\n",
    "\"needn't\": \"need not\",\n",
    "\"needn't've\": \"need not have\",\n",
    "\"o'clock\": \"of the clock\",\n",
    "\"oughtn't\": \"ought not\",\n",
    "\"oughtn't've\": \"ought not have\",\n",
    "\"shan't\": \"shall not\",\n",
    "\"sha'n't\": \"shall not\",\n",
    "\"shan't've\": \"shall not have\",\n",
    "\"she'd\": \"she had\",\n",
    "\"she'd've\": \"she would have\",\n",
    "\"she'll\": \"she will\",\n",
    "\"she'll've\": \"she will have\",\n",
    "\"she's\": \"she is\",\n",
    "\"should've\": \"should have\",\n",
    "\"shouldn't\": \"should not\",\n",
    "\"shouldn't've\": \"should not have\",\n",
    "\"so've\": \"so have\",\n",
    "\"so's\": \"so is\",\n",
    "\"that'd\": \"that had\",\n",
    "\"that'd've\": \"that would have\",\n",
    "\"that's\": \"that is\",\n",
    "\"there'd\": \"there would\",\n",
    "\"there'd've\": \"there would have\",\n",
    "\"there's\": \"there is\",\n",
    "\"they'd\": \"they had / they would\",\n",
    "\"they'd've\": \"they would have\",\n",
    "\"they'll\": \"they will\",\n",
    "\"they'll've\": \"they will have\",\n",
    "\"they're\": \"they are\",\n",
    "\"they've\": \"they have\",\n",
    "\"to've\": \"to have\",\n",
    "\"wasn't\": \"was not\",\n",
    "\"we'd\": \"we would\",\n",
    "\"we'd've\": \"we would have\",\n",
    "\"we'll\": \"we will\",\n",
    "\"we'll've\": \"we will have\",\n",
    "\"We're\": \"we are\",\n",
    "\"we're\": \"we are\",\n",
    "\"we've\": \"we have\",\n",
    "\"weren't\": \"were not\",\n",
    "\"what'll\": \"what will\",\n",
    "\"what'll've\": \"what will have\",\n",
    "\"what're\": \"what are\",\n",
    "\"what's\": \"what is\",\n",
    "\"what've\": \"what have\",\n",
    "\"when's\": \"when is\",\n",
    "\"when've\": \"when have\",\n",
    "\"where'd\": \"where did\",\n",
    "\"where's\": \"where is\",\n",
    "\"where've\": \"where have\",\n",
    "\"who'll\": \"who will\",\n",
    "\"who'll've\": \"who will have\",\n",
    "\"who's\": \"who has\",\n",
    "\"who've\": \"who have\",\n",
    "\"why's\": \"why is\",\n",
    "\"why've\": \"why have\",\n",
    "\"will've\": \"will have\",\n",
    "\"won't\": \"will not\",\n",
    "\"won't've\": \"will not have\",\n",
    "\"would've\": \"would have\",\n",
    "\"wouldn't\": \"would not\",\n",
    "\"wouldn't've\": \"would not have\",\n",
    "\"y'all\": \"you all\",\n",
    "\"y'all'd\": \"you all would\",\n",
    "\"y'all'd've\": \"you all would have\",\n",
    "\"y'all're\": \"you all are\",\n",
    "\"y'all've\": \"you all have\",\n",
    "\"you'd\": \"you would\",\n",
    "\"you'd've\": \"you would have\",\n",
    "\"you'll\": \"you will\",\n",
    "\"you'll've\": \"you will have\",\n",
    "\"you're\": \"you are\",\n",
    "\"you've\": \"you have\"\n",
    "}\n",
    "\n",
    "contractions_re = re.compile('(%s)' % '|'.join(contractions.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### function to clean the comment_text data\n",
    "def expand_contractions(s, contractions_dict=contractions):\n",
    "    def replace(match):\n",
    "        return contractions_dict[match.group(0)]\n",
    "    return contractions_re.sub(replace, s)\n",
    "\n",
    "\n",
    "def clean_data(text):\n",
    "    text = expand_contractions(text, contractions)\n",
    "    text = str(text).lower()\n",
    "    text = re.sub(r'\\w+:\\/{2}[\\d\\w-]+(\\.[\\d\\w-]+)*(?:(?:\\/[^\\s/]*))*', ' url ', text)\n",
    "    text = re.sub('(!+)', '!', text)\n",
    "    text = re.sub('(\\?+)', '?', text)\n",
    "    text = re.sub('(\\s+)', ' ', text)\n",
    "    text = re.sub('(\\\"+)', ' \\\" ', text)\n",
    "    text = re.sub('(\\.+)', '\\.', text)\n",
    "    text = re.sub('(<+)', ' < .', text)\n",
    "    text = re.sub('(>+)', ' > .', text)\n",
    "    text = re.sub('\\n', ' ', text)\n",
    "    text = re.sub('\\t', ' ', text)\n",
    "    text = text.replace(\"\\\\\", \" \")\n",
    "    text = text.replace(\"-\", \" \")\n",
    "    text = text.replace(\"â€”\", \" \")\n",
    "    text = text.replace(\"/\", \" \")\n",
    "    text = text.replace(\"_\", \" \")\n",
    "    text = re.sub('([.,!?()])', r' \\1 ', text)\n",
    "    text = re.sub('\\s{2,}', ' ', text)\n",
    "    text = re.sub('[^A-Za-z0-9\\?\\!\\.<>,\\s]+', '', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_pad_slice_column_to_int(df, row_name, vocab_dict, max_sequence_length, stop_words):\n",
    "    processed_row = []\n",
    "    \n",
    "    for i, row in df.iterrows():\n",
    "        text = clean_data(str(row[row_name]))\n",
    "        text_arr = text.split(\" \")\n",
    "        text_arr = [w for w in text_arr if not w in stop_words] \n",
    "        \n",
    "        if len(text_arr) > max_sequence_length:\n",
    "            text_arr = text_arr[:max_sequence_length]\n",
    "        elif len(text_arr) < max_sequence_length:\n",
    "            pad_to_add = max_sequence_length-len(text_arr)\n",
    "            text_arr.extend(['<pad>' for i in range(pad_to_add)])\n",
    "        \n",
    "        for i in range(len(text_arr)):\n",
    "            if text_arr[i] in vocab_dict:\n",
    "                text_arr[i] = vocab_dict[text_arr[i]]\n",
    "            else:\n",
    "                text_arr[i] = vocab_dict['<unk>']\n",
    "            \n",
    "        processed_row.append(np.array(text_arr))\n",
    "        \n",
    "    df[\"processed_row\"] = processed_row\n",
    "    return df\n",
    "\n",
    "def create_vocab_from_df_column(df, row_name):\n",
    "    vocab_dict = {'<pad>' : 0,\n",
    "                  '<unk>' : 1}\n",
    "    index = 2\n",
    "    \n",
    "    for i, row in df.iterrows():\n",
    "        text = clean_data(str(row[row_name]))\n",
    "        text_arr = text.split(\" \")\n",
    "        for word in text_arr:\n",
    "            if word in vocab_dict:\n",
    "                continue\n",
    "            else:\n",
    "                vocab_dict[word] = index\n",
    "                index += 1\n",
    "                \n",
    "    return vocab_dict\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_dict = create_vocab_from_df_column(df, 'review')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = tokenize_pad_slice_column_to_int(df, 'review', vocab_dict, 300, stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>review</th>\n",
       "      <th>label</th>\n",
       "      <th>processed_row</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>test</td>\n",
       "      <td>Once again Mr. Costner has dragged out a movie...</td>\n",
       "      <td>0</td>\n",
       "      <td>[4, 5, 6, 8, 11, 13, 14, 16, 5, 17, 20, 21, 22...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>test</td>\n",
       "      <td>This is an example of why the majority of acti...</td>\n",
       "      <td>0</td>\n",
       "      <td>[123, 124, 125, 126, 5, 128, 129, 24, 60, 130,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>test</td>\n",
       "      <td>First of all I hate those moronic rappers, who...</td>\n",
       "      <td>0</td>\n",
       "      <td>[236, 237, 239, 240, 24, 241, 242, 244, 245, 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>test</td>\n",
       "      <td>Not even the Beatles could write songs everyon...</td>\n",
       "      <td>0</td>\n",
       "      <td>[324, 325, 116, 326, 327, 328, 329, 24, 330, 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>test</td>\n",
       "      <td>Brass pictures (movies is not a fitting word f...</td>\n",
       "      <td>0</td>\n",
       "      <td>[441, 442, 122, 336, 443, 444, 122, 60, 201, 4...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   type                                             review  label  \\\n",
       "0  test  Once again Mr. Costner has dragged out a movie...      0   \n",
       "1  test  This is an example of why the majority of acti...      0   \n",
       "2  test  First of all I hate those moronic rappers, who...      0   \n",
       "3  test  Not even the Beatles could write songs everyon...      0   \n",
       "4  test  Brass pictures (movies is not a fitting word f...      0   \n",
       "\n",
       "                                       processed_row  \n",
       "0  [4, 5, 6, 8, 11, 13, 14, 16, 5, 17, 20, 21, 22...  \n",
       "1  [123, 124, 125, 126, 5, 128, 129, 24, 60, 130,...  \n",
       "2  [236, 237, 239, 240, 24, 241, 242, 244, 245, 2...  \n",
       "3  [324, 325, 116, 326, 327, 328, 329, 24, 330, 2...  \n",
       "4  [441, 442, 122, 336, 443, 444, 122, 60, 201, 4...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create batch generator\n",
    "df_train = df[df['type'] == 'train'].sample(frac=1).reset_index(drop=True).drop(['type'], axis = 1)\n",
    "df_train.label = df_train.label.astype('float64')\n",
    "\n",
    "df_test = df[df['type'] == 'test'].sample(frac=1).reset_index(drop=True).drop(['type'], axis = 1)\n",
    "df_test.label = df_test.label.astype('float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Input Shape: torch.Size([25000, 300]) Train Target Shape: torch.Size([25000, 1])\n",
      "Test Input Shape: torch.Size([25000, 300]) Test Target Shape: torch.Size([25000, 1])\n"
     ]
    }
   ],
   "source": [
    "X_train = df_train.processed_row.tolist()\n",
    "X_train = torch.from_numpy(np.vstack(X_train))\n",
    "\n",
    "X_test = df_test.processed_row.tolist()\n",
    "X_test = torch.from_numpy(np.vstack(X_test))\n",
    "\n",
    "y_train = df_train.label.tolist()\n",
    "#y_train = torch.from_numpy(np.vstack(y_train), dtype=torch.long)\n",
    "y_train = torch.tensor(np.vstack(y_train), dtype=torch.float)\n",
    "\n",
    "y_test = df_test.label.to_list()\n",
    "#y_test = torch.from_numpy(np.vstack(y_test))\n",
    "y_test = torch.tensor(np.vstack(y_train), dtype=torch.float)\n",
    "\n",
    "print(\"Train Input Shape: {} Train Target Shape: {}\".format(X_train.shape, y_train.shape))\n",
    "print(\"Test Input Shape: {} Test Target Shape: {}\".format(X_test.shape, y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = data_utils.TensorDataset(X_train, y_train)\n",
    "train_loader = data_utils.DataLoader(train, batch_size=50, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = data_utils.TensorDataset(X_test, y_test)\n",
    "test_loader = data_utils.DataLoader(train, batch_size=50, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_wv_matrix(vocab_dict):\n",
    "    print ('... Loading Word Vectors')\n",
    "    word_vectors = KeyedVectors.load_word2vec_format(\"./models/GoogleNews-vectors-negative300.bin\", binary=True)\n",
    "    wv_matrix = []\n",
    "    count = 0\n",
    "    print ('... Finish Loading Word Vectors')\n",
    "    \n",
    "    for each in vocab_dict.items():\n",
    "        count += 1\n",
    "        \n",
    "        word = str(each[0]).lower()\n",
    "        index = int(each[1])\n",
    "        \n",
    "        if word in word_vectors.vocab:\n",
    "            wv_matrix.append(word_vectors.word_vec(word))\n",
    "        else:\n",
    "            wv_matrix.append(np.random.uniform(-0.01, 0.01, 300).astype(\"float32\"))\n",
    "        \n",
    "        if count %10000 == 0:\n",
    "            print (\"On Index {}\".format(count))\n",
    "            \n",
    "    ### Add Unknown Token\n",
    "    wv_matrix.append(np.random.uniform(-0.01, 0.01, 300).astype(\"float32\"))\n",
    "    ### Add Pad Token\n",
    "    wv_matrix.append(np.zeros(300).astype(\"float32\"))\n",
    "    print ('... Finished Creating Matrix')\n",
    "    \n",
    "    del(word_vectors)\n",
    "    \n",
    "    return np.array(wv_matrix)\n",
    "\n",
    "def create_emb_layer(weights_matrix, non_trainable=False):\n",
    "    num_embeddings, embedding_dim = weights_matrix.shape\n",
    "    emb_layer = nn.Embedding(num_embeddings, embedding_dim)\n",
    "    emb_layer.weight.data.copy_(torch.from_numpy(weights_matrix))\n",
    "    \n",
    "    if non_trainable:\n",
    "        emb_layer.weight.requires_grad = False\n",
    "\n",
    "    return emb_layer, num_embeddings, embedding_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... Loading Word Vectors\n",
      "... Finish Loading Word Vectors\n",
      "On Index 10000\n",
      "On Index 20000\n",
      "On Index 30000\n",
      "On Index 40000\n",
      "On Index 50000\n",
      "On Index 60000\n",
      "On Index 70000\n",
      "On Index 80000\n",
      "On Index 90000\n",
      "On Index 100000\n",
      "... Finished Creating Matrix\n"
     ]
    }
   ],
   "source": [
    "### Get Word Vector Matrix\n",
    "wv_matrix = create_wv_matrix(vocab_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50, 300, 300])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Test the Embedding Layer\n",
    "emb_layer, num_embeddings, embedding_dim = create_emb_layer(wv_matrix)\n",
    "batch = next(iter(train_loader))\n",
    "emb_layer(batch[0]).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, weights_matrix):\n",
    "        super(CNN, self).__init__()\n",
    "        \n",
    "        ### Embedding Layer\n",
    "        self.embedding, num_embeddings, embedding_dim = create_emb_layer(weights_matrix, True)\n",
    "       \n",
    "        ### Convolution Layer 1\n",
    "        self.conv1 = nn.Sequential(         # input shape (1, 300, 300)\n",
    "            nn.Conv2d(\n",
    "                in_channels=1,              # input height\n",
    "                out_channels=3,             # n_filters\n",
    "                kernel_size=10,             # filter size\n",
    "                stride=2,                   # filter movement/step\n",
    "                padding=0,                  # if want same width and length of this image after Conv2d, padding=(kernel_size-1)/2 if stride=1\n",
    "            ),                              # output shape (3, 146, 146)\n",
    "            nn.ReLU(),                      # activation\n",
    "            nn.MaxPool2d(kernel_size=2),    # (146-2 / 2) choose max value in 2x2 area, output shape (3, 73, 73)\n",
    "        )\n",
    "        \n",
    "        ### Convolution Layer 2\n",
    "        self.conv2 = nn.Sequential(        # input shape (3, 73, 73)\n",
    "            nn.Conv2d(3, 3, 5, 2, 1),      # output shape (3, 36, 36)\n",
    "            nn.ReLU(),                     # activation\n",
    "            nn.MaxPool2d(2),               # output shape (3, 18, 18)\n",
    "        )\n",
    "            \n",
    "        ### Fully Connected Layer 3\n",
    "        self.FC1 = nn.Linear(3 * 18 * 18, 800)\n",
    "        \n",
    "        ### Fully Connected Layer 4\n",
    "        self.FC2 = nn.Linear(800, 200)\n",
    "        \n",
    "        # Output 2 classes\n",
    "        self.out = nn.Linear(200, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        #print (x.shape)\n",
    "        x = x.unsqueeze(1)\n",
    "        #print (x.shape)\n",
    "        x = self.conv1(x)\n",
    "        #print (x.shape)\n",
    "        x = self.conv2(x)\n",
    "        #print (x.shape)\n",
    "        x = x.view(x.size(0), -1)           # flatten the output of conv2\n",
    "        x = F.relu(self.FC1(x))\n",
    "        #print (x.shape)\n",
    "        x = F.relu(self.FC2(x))\n",
    "        #print (x.shape)\n",
    "        output =  torch.sigmoid(self.out(x))\n",
    "        #print (output.shape)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Hyperparameters\n",
    "num_epochs = 30\n",
    "cnn_classifier = CNN(weights_matrix=wv_matrix)\n",
    "\n",
    "optimizer = optim.Adam(cnn_classifier.parameters())\n",
    "criterion = nn.MSELoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_accuracy(preds, y):\n",
    "    #round predictions to the closest integer\n",
    "    rounded_preds = torch.round(preds)\n",
    "    correct = (rounded_preds == y).float() #convert into float for division \n",
    "    acc = correct.sum() / len(correct)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    count = 1\n",
    "    model.train()\n",
    "    \n",
    "    for batch in iterator:\n",
    "        count +=1\n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(batch[0])\n",
    "        loss = criterion(predictions, batch[1])\n",
    "        acc = binary_accuracy(predictions, batch[1])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in iterator:\n",
    "            predictions = model(batch[0])\n",
    "            loss = criterion(predictions, batch[1])\n",
    "            acc = binary_accuracy(predictions, batch[1])\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 0, TIME: 1:46\n",
      "Train Loss: 0.2065124011039734. Train Acc: 65.8360002875328\n",
      "Test Loss: 0.16567027720808983. Test Acc: 75.89199985265732\n",
      "EPOCH: 1, TIME: 1:47\n",
      "Train Loss: 0.16196294552087784. Train Acc: 76.4119998574257\n",
      "Test Loss: 0.1575906427949667. Test Acc: 77.41199966669082\n",
      "EPOCH: 2, TIME: 1:46\n",
      "Train Loss: 0.15299007812142373. Train Acc: 77.94399963617325\n",
      "Test Loss: 0.1421374605000019. Test Acc: 79.69199942350387\n",
      "EPOCH: 3, TIME: 1:46\n",
      "Train Loss: 0.14726545952260495. Train Acc: 78.7439996600151\n",
      "Test Loss: 0.13765579323470592. Test Acc: 80.35999960899353\n",
      "EPOCH: 4, TIME: 1:49\n",
      "Train Loss: 0.14309378159046174. Train Acc: 79.46399945020676\n",
      "Test Loss: 0.13289373557269574. Test Acc: 81.19999957084656\n",
      "EPOCH: 5, TIME: 1:53\n",
      "Train Loss: 0.14042587108165025. Train Acc: 79.82799965143204\n",
      "Test Loss: 0.1405327669903636. Test Acc: 80.16399949789047\n",
      "EPOCH: 6, TIME: 1:57\n",
      "Train Loss: 0.13708722899854184. Train Acc: 80.17599952220917\n",
      "Test Loss: 0.13045398165285588. Test Acc: 81.69199948310852\n",
      "EPOCH: 7, TIME: 1:54\n",
      "Train Loss: 0.13355617213994264. Train Acc: 81.0079995393753\n",
      "Test Loss: 0.1253583028540015. Test Acc: 82.62799936532974\n",
      "EPOCH: 8, TIME: 1:49\n",
      "Train Loss: 0.13095451901853083. Train Acc: 81.37199951410294\n",
      "Test Loss: 0.12147546298801899. Test Acc: 83.23199943304061\n",
      "EPOCH: 9, TIME: 1:53\n",
      "Train Loss: 0.12705253472179173. Train Acc: 82.16799948215484\n",
      "Test Loss: 0.1185844786465168. Test Acc: 83.64799938201905\n",
      "EPOCH: 10, TIME: 1:53\n",
      "Train Loss: 0.12274619901925325. Train Acc: 82.96399952173233\n",
      "Test Loss: 0.11165414685755969. Test Acc: 84.8479995250702\n",
      "EPOCH: 11, TIME: 1:51\n",
      "Train Loss: 0.11874387768656015. Train Acc: 83.57199954986572\n",
      "Test Loss: 0.12375792707502842. Test Acc: 82.66399934291839\n",
      "EPOCH: 12, TIME: 1:48\n",
      "Train Loss: 0.11218995458632708. Train Acc: 84.57199938297272\n",
      "Test Loss: 0.10140010717138648. Test Acc: 86.43999942541123\n",
      "EPOCH: 13, TIME: 1:52\n",
      "Train Loss: 0.10546547604911029. Train Acc: 85.84799947738647\n",
      "Test Loss: 0.09520375252142549. Test Acc: 87.30799964666367\n",
      "EPOCH: 14, TIME: 1:53\n",
      "Train Loss: 0.0978970073312521. Train Acc: 87.00399962663651\n",
      "Test Loss: 0.08731101973727345. Test Acc: 89.2999997138977\n",
      "EPOCH: 15, TIME: 1:52\n",
      "Train Loss: 0.08929051754809916. Train Acc: 88.37199960947036\n",
      "Test Loss: 0.07423524430021644. Test Acc: 90.7279997229576\n",
      "EPOCH: 16, TIME: 1:54\n",
      "Train Loss: 0.08143461851030588. Train Acc: 89.47199952602386\n",
      "Test Loss: 0.07199061095714569. Test Acc: 90.73199961185455\n",
      "EPOCH: 17, TIME: 2:2\n",
      "Train Loss: 0.07293808295764029. Train Acc: 90.75199964046479\n",
      "Test Loss: 0.06147245278954506. Test Acc: 92.59199974536895\n",
      "EPOCH: 18, TIME: 2:4\n",
      "Train Loss: 0.06375640298798681. Train Acc: 92.13599972724914\n",
      "Test Loss: 0.052784631355199965. Test Acc: 93.63199976682664\n",
      "EPOCH: 19, TIME: 2:5\n",
      "Train Loss: 0.05759030594117939. Train Acc: 92.97599979639052\n",
      "Test Loss: 0.052628597605507824. Test Acc: 93.81199966669082\n",
      "EPOCH: 20, TIME: 2:2\n",
      "Train Loss: 0.05170646939752623. Train Acc: 93.85599973201751\n",
      "Test Loss: 0.03860832010116428. Test Acc: 95.6319998383522\n",
      "EPOCH: 21, TIME: 2:3\n",
      "Train Loss: 0.045923345519928264. Train Acc: 94.65599981546401\n",
      "Test Loss: 0.04509273533616215. Test Acc: 94.60799976587295\n",
      "EPOCH: 22, TIME: 2:3\n",
      "Train Loss: 0.04223505823640153. Train Acc: 95.01199979782105\n",
      "Test Loss: 0.036525971938390286. Test Acc: 95.79599992036819\n",
      "EPOCH: 23, TIME: 2:7\n",
      "Train Loss: 0.03830400436441414. Train Acc: 95.58799980878831\n",
      "Test Loss: 0.03143903755233623. Test Acc: 96.55199996232986\n",
      "EPOCH: 24, TIME: 2:4\n",
      "Train Loss: 0.035552093328442426. Train Acc: 95.91599988937378\n",
      "Test Loss: 0.02887249762073043. Test Acc: 96.89599999189376\n",
      "EPOCH: 25, TIME: 2:5\n",
      "Train Loss: 0.034273616860155016. Train Acc: 96.05199991464615\n",
      "Test Loss: 0.029975494779941072. Test Acc: 96.58000010251999\n",
      "EPOCH: 26, TIME: 2:6\n",
      "Train Loss: 0.032454011725931195. Train Acc: 96.32399998903274\n",
      "Test Loss: 0.029846026459825224. Test Acc: 96.61200003623962\n",
      "EPOCH: 27, TIME: 2:5\n",
      "Train Loss: 0.031527761894154537. Train Acc: 96.4199998497963\n",
      "Test Loss: 0.026239062176755396. Test Acc: 97.1560001373291\n",
      "EPOCH: 28, TIME: 2:6\n",
      "Train Loss: 0.0297440160807746. Train Acc: 96.68400013446808\n",
      "Test Loss: 0.029346141899150097. Test Acc: 96.70400000810623\n",
      "EPOCH: 29, TIME: 2:6\n",
      "Train Loss: 0.03268120385246948. Train Acc: 96.25999990701676\n",
      "Test Loss: 0.024941952820263395. Test Acc: 97.26000020503997\n"
     ]
    }
   ],
   "source": [
    "### Training Loop\n",
    "best_test_loss = float('inf')\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss, train_acc = train(cnn_classifier, train_loader, optimizer, criterion)\n",
    "    test_loss, test_acc = evaluate(cnn_classifier, test_loader, criterion)\n",
    "    \n",
    "    end_time = time.time()\n",
    "\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    if test_loss < best_test_loss:\n",
    "        best_test_loss = test_loss\n",
    "        torch.save(cnn_classifier.state_dict(), 'tut4-model.pt')\n",
    "    \n",
    "    print(\"EPOCH: {}, TIME: {}:{}\".format(epoch, epoch_mins, epoch_secs))\n",
    "    print(\"Train Loss: {}. Train Acc: {}\".format(train_loss, train_acc*100))\n",
    "    print(\"Test Loss: {}. Test Acc: {}\".format(test_loss, test_acc*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CNN(\n",
       "  (embedding): Embedding(108875, 300)\n",
       "  (conv1): Sequential(\n",
       "    (0): Conv2d(1, 3, kernel_size=(10, 10), stride=(2, 2))\n",
       "    (1): ReLU()\n",
       "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (conv2): Sequential(\n",
       "    (0): Conv2d(3, 3, kernel_size=(5, 5), stride=(2, 2), padding=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (FC1): Linear(in_features=972, out_features=800, bias=True)\n",
       "  (FC2): Linear(in_features=800, out_features=200, bias=True)\n",
       "  (out): Linear(in_features=200, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn_classifier.load_state_dict(torch.load('./tut4-model.pt'))\n",
    "cnn_classifier.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_new_text(model, text):\n",
    "    predict_df = pd.DataFrame( {'Message' : [text_to_classify]})\n",
    "    predict_df = tokenize_pad_slice_column_to_int(predict_df, 'Message', vocab_dict, 300, stop_words)\n",
    "\n",
    "    predict = predict_df.processed_row.tolist()\n",
    "    predict = torch.from_numpy(np.vstack(predict))\n",
    "    predict = np.round((model(predict)[0][0]).detach().numpy())\n",
    "    \n",
    "    if predict == 0:\n",
    "        print (\"Negative Review\")\n",
    "    if predict == 1:\n",
    "        print (\"Positive Review\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive Review\n"
     ]
    }
   ],
   "source": [
    "text_to_classify = \"\"\"This movie is the beginning of the culmination of Marvel's masterfully woven cinematic universe. \n",
    "                    Beginning back in 2008 with iron man, we are finally seeing the results of all the movies have been pointing to; \n",
    "                    and it did not disappoint. Thanos is a complex villain, with deeper and more interesting desires than just \"world domination.\" \n",
    "                    The dilemmas all the characters face in this movie (both the heroes and the villains) are truly thought provoking and \n",
    "                    leave you on the edge of your seat. No other set of movies has beeen so involved, so expanded, and encompassed so many story\n",
    "                    lines/characters and previous movies. The sheer amount of star power alone in this film is insane; and they do a masterful\n",
    "                    job of weaving all these unique and various characters into a common storyline.\"\"\"\n",
    "\n",
    "predict_new_text(cnn_classifier, text_to_classify)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
