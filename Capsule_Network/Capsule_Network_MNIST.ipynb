{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Capsnet: \"Dynamic Routing Between Capsules\"</h1>\n",
    "<h4><span style=\"text-decoration: underline;\">Original Paper:</span> Geoffrey E. Hinton,&nbsp;Sara Sabour,&nbsp; Nicholas Frosst</h4>\n",
    "<h4><span style=\"text-decoration: underline;\">Notebook Author:</span>&nbsp; Brandon Lwowski</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>I. Introduction</h2>\n",
    "<p>Geoffrey Hinton and his colleagues developed a new architecture for neural nets, called Capsule Networks, to tackle some of the negative issues present with the traditional CNN. The Convolutional Neural Network is commonly used in tasks such as computer vision and text classification. VGG-16, Inception, and ResNet-50 are all architectures that are extremely successful in the domain of computer vision and all use multiple convolutional layers in their networks.</p>\n",
    "<p>Even with the large amount of success that CNN's have earned, there are drawbacks present that limit their overall ability. CNN's do not store spatial or orientational relationships between features. The most common example is the human face. As long as the image has 2 eyes, a nose, and a mouth, the CNN will classify it a face, regardless of if the mouth is above the eyes, or the nose is by the ears.&nbsp;</p>\n",
    "<p>The Capsule Network was Hinton&rsquo;s attempt to fix these known issues of CNN's. The \"capsules\" in this architecture learn spatial and orientational features, which will be discussed later in this notebook. In this notebook you will see my implementation of a Capsule Network on the MNIST data set. The network is trained on data that has not been augmented and tested on images that have been scaled and rotated. You will see that the CapsNet does well on the scaled data but, still struggles to solve the rotational invariance issue with traditional CNNs.</p>\n",
    "<img src = \"./images/capsnet.png\">\n",
    "<h2>II. Background</h2>\n",
    "<p>&nbsp;</p>\n",
    "<p>What are Capsules? You can find a detailed explanation in Hinton&rsquo;s first paper \"<a href=\"http://www.cs.toronto.edu/~fritz/absps/transauto6.pdf\">Transforming Auto-encoders</a>\" but I will do my best to paraphrase here. A capsule is a set of neurons that produce a vector. This differs from a typical neural network, where the output is a scalar. This vector can represent the size, orientation, or even the hue of the image once trained.</p>\n",
    "<p>&nbsp;</p>\n",
    "<img src = \"./images/capsule.png\">\n",
    "<p>In the image above you will see 2 different types of capsules. The red capsules are created using 3 neurons and the green capsules are created using 4 neurons. The red capsules are used as recognition units and the green capsules are used as generation units (This will be discussed further down in the notebook).&nbsp; Recognition units, similar to the hidden later in a general artificial neural network, are used to compute the X-position, Y-position and the probability that the visual feature is present in the input image. The generation units receive the X-position plus the change in X and the Y-position plus the change in Y and then are multiplied by the probability to generate the actual output. The distance between the actual output and target output are then calculated and used to adjust the capsules weights. By multiplying the output of the generation units by the probability, inactive capsules will not affect the actual output.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>III. Implementation</h2>\n",
    "<p>Below you will find my pytorch implementation of capsule networks and the dynamic routing algorithm provided by Hinton.&nbsp;</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Import Libraries\n",
    "\n",
    "### pytorch libraries needed\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "import numpy as np               ### Matrix Math\n",
    "import matplotlib.pyplot as plt  ### Data visualizations\n",
    "\n",
    "from tqdm import tqdm            ### Create progress bar\n",
    "import gc                        ### Garbage collect to save memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f3c49e04310>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Hyperparameters\n",
    "USE_CUDA = True if torch.cuda.is_available() else False\n",
    "BATCH_SIZE = 128\n",
    "N_EPOCHS = 30            \n",
    "LEARNING_RATE = 0.0001\n",
    "MOMENTUM = 0.7\n",
    "\n",
    "random_seed = 1\n",
    "torch.backends.cudnn.enabled = False\n",
    "torch.manual_seed(random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "55"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>III(a). Data and Data Analysis</h2>\n",
    "<p>The following four functions are used to create train and test data. For this experiment of the capsule network, I plan to train the model on data that has not been augmented. It will be trained on the original 29x29 MNIST data set. The goal of this notebook is to investigate how well the model can classify MNIST data that has been rotated or scaled. The <font face=\"courier\">test_loader</font> is a base line; these images are not augmented.  <font face=\"courier\">test_loader_rotate</font> is a test set of MNIST images randomly rotated -75 degrees to 75 degrees. Lastly, the  <font face=\"courier\">test_loader_scale_half</font> was resized, so the image is half the size and padded to match the 28x28 dimensions. Below those 4 functions you will find examples of training set, test set original, test set rotated, and test set scaled&nbsp;</p>\n",
    "<img src = \"./images/MNIST.png\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Get MNIST Data\n",
    "\n",
    "#train = True pulls from training set train = False pulls from testing data\n",
    "#Compose() chains multiple transformations together\n",
    "#ToTensor() transforms PIL image or numpy array to torch tensor\n",
    "#Normalize() normalize a tensor image with mean and standard deviation (mean, std)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "  torchvision.datasets.MNIST('./files', train=True, download=True,\n",
    "                             transform=torchvision.transforms.Compose([\n",
    "                               torchvision.transforms.Resize((28, 28)),\n",
    "                               torchvision.transforms.ToTensor(),\n",
    "                               torchvision.transforms.Normalize(\n",
    "                                 (0.1307,), (0.3081,))\n",
    "                             ])),\n",
    "  batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "  torchvision.datasets.MNIST('./files', train=True, download=True,\n",
    "                             transform=torchvision.transforms.Compose([\n",
    "                               torchvision.transforms.Resize((28, 28)),\n",
    "                               torchvision.transforms.ToTensor(),\n",
    "                               torchvision.transforms.Normalize(\n",
    "                                 (0.1307,), (0.3081,))\n",
    "                             ])),\n",
    "  batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "test_loader_rotate = torch.utils.data.DataLoader(\n",
    "  torchvision.datasets.MNIST('./files', train=False, download=True,\n",
    "                             transform=torchvision.transforms.Compose([\n",
    "                               torchvision.transforms.RandomRotation((-75,75)),\n",
    "                               torchvision.transforms.Resize((28, 28)),\n",
    "                               torchvision.transforms.ToTensor(),\n",
    "                               torchvision.transforms.Normalize((0.1307,), (0.3081,))\n",
    "                             ])),\n",
    "  batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "test_loader_scale_half = torch.utils.data.DataLoader(\n",
    "  torchvision.datasets.MNIST('./files', train=False, download=True,\n",
    "                             transform=torchvision.transforms.Compose([\n",
    "                                transforms.Resize(20),\n",
    "                                transforms.Pad(padding=4, padding_mode='edge'),\n",
    "                                transforms.ToTensor(),\n",
    "                                transforms.Normalize((0.1307,), (0.3081,))\n",
    "                             ])),\n",
    "  batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index: 0\n",
      "Example Shape: torch.Size([128, 1, 28, 28])\n",
      "Target Shape : torch.Size([128])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<module 'matplotlib.pyplot' from '/home/ubuntu/.local/lib/python3.7/site-packages/matplotlib/pyplot.py'>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x576 with 6 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "### Train Data\n",
    "examples = enumerate(train_loader)\n",
    "batch_idx, (example_data, example_targets) = next(examples)\n",
    "\n",
    "print (\"Index:\", batch_idx)\n",
    "print (\"Example Shape:\", example_data.shape)\n",
    "print (\"Target Shape :\", example_targets.shape)\n",
    "\n",
    "fig = plt.figure(figsize=(15, 8))\n",
    "for i in range(6):\n",
    "    plt.subplot(1,6, i+1)\n",
    "    plt.imshow(example_data[i][0], cmap='gray', interpolation='none')\n",
    "    plt.title(\"Ground Truth: {}\".format(example_targets[i]))\n",
    "plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index: 0\n",
      "Example Shape: torch.Size([128, 1, 28, 28])\n",
      "Target Shape : torch.Size([128])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<module 'matplotlib.pyplot' from '/home/ubuntu/.local/lib/python3.7/site-packages/matplotlib/pyplot.py'>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x576 with 6 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "### Test Data Rotate\n",
    "examples = enumerate(test_loader_rotate)\n",
    "batch_idx, (example_data, example_targets) = next(examples)\n",
    "\n",
    "print (\"Index:\", batch_idx)\n",
    "print (\"Example Shape:\", example_data.shape)\n",
    "print (\"Target Shape :\", example_targets.shape)\n",
    "\n",
    "fig = plt.figure(figsize=(15, 8))\n",
    "for i in range(6):\n",
    "    plt.subplot(1,6, i+1)\n",
    "    plt.imshow(example_data[i][0], cmap='gray', interpolation='none')\n",
    "    plt.title(\"Ground Truth: {}\".format(example_targets[i]))\n",
    "plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index: 0\n",
      "Example Shape: torch.Size([128, 1, 28, 28])\n",
      "Target Shape : torch.Size([128])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<module 'matplotlib.pyplot' from '/home/ubuntu/.local/lib/python3.7/site-packages/matplotlib/pyplot.py'>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x576 with 6 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "### Test Data Scale\n",
    "examples = enumerate(test_loader_scale_half)\n",
    "batch_idx, (example_data, example_targets) = next(examples)\n",
    "\n",
    "print (\"Index:\", batch_idx)\n",
    "print (\"Example Shape:\", example_data.shape)\n",
    "print (\"Target Shape :\", example_targets.shape)\n",
    "\n",
    "fig = plt.figure(figsize=(15, 8))\n",
    "for i in range(6):\n",
    "    plt.subplot(1,6, i+1)\n",
    "    plt.imshow(example_data[i][0], cmap='gray', interpolation='none')\n",
    "    plt.title(\"Ground Truth: {}\".format(example_targets[i]))\n",
    "plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20700"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>III(b). Design</h2>\n",
    "<p>The topology of a Capsule Network has 4 major components that make up the CapsNet architecture. The convolutional layer, primary caps layer, digit caps layer, and the decoder.&nbsp; Each of the 4 components will have a detailed explanation further down in the notebook.&nbsp;</p>\n",
    "<p>In order to discuss how the CapsNet works, it is important to understand the Hinton&rsquo;s new activation function, called the squash function, the loss function for the CapsNet, and the Dynamic Routing algorithm used in his paper.</p>\n",
    "<p>Below you will find the equation created by Hinton called the squash function. Typically, in CNNs, you will see activation functions like relu in order to create non-linearity in the network. The objective of the squash function is to take a vector as an input and \"squash\" it to have a length of no more than 1. By squashing the vector, it can now be interpreted as a probability that the feature is present in the image.</p>\n",
    "<p>Looking at the formula it is key to note that the subscript 'j' represents capsule j and s <sub>j</sub> is the input vector.&nbsp;</p>\n",
    "<img src = \"./images/squash.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def squash(s_j):\n",
    "    squared_norm = (s_j ** 2).sum(-1, keepdim=True)\n",
    "    v_j = squared_norm * s_j / ((1. + squared_norm) * torch.sqrt(squared_norm))\n",
    "    return v_j"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Margin Loss from CapsNet Paper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"./images/marginloss.JPG\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>The next important part of the CapsNet to discuss is the loss function used by Hinton in his paper. He calls it the \"margin loss for digit existence\". In order to understand this complicated equation, it is easier to break it into pieces. What the margin loss function is trying to do is calculate the loss for every class \"k\". The term L<sub>k</sub>is the calculated loss for class k. The losses for each k will be summed and averaged to calculate the final loss for the current batch.&nbsp;</p>\n",
    "<p>The next import part of the equation in T<sub>k</sub>.   T<sub>k</sub> is the correct label for the input v<sub>k</sub>. T<sub>k</sub> will always equal 1 if DigitCap is correct and will equal 0 when it is incorrect.&nbsp;</p>\n",
    "<p>The max functions can be defined as relu activations. The max function on the left will produce a 0 loss when the prediction has a greater probability than 0.9 and the max function on the right will produce a 0 loss with a probability less than 0.1.&nbsp;</p>\n",
    "<p>The last piece to discuss it the lambda times 1 - T<sub>c</sub>. When T<sub>c</sub> is correct then the entire expression goes to 0. When T<sub>c</sub> is incorrect the expression goes to lambda. In Hinton&rsquo;s implementation, he uses a lambda value of 0.5.&nbsp;</p>\n",
    "<p>To simplify the equation, the left side of the equation will be calculated if the prediction is correct and the right side of the equation will be calculated if the prediction is incorrect.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def margin_loss(v_k, T_k, size_average=True):\n",
    "        #v_k shape = [batch_size, number of digits, length of digit caps, 1 batch]\n",
    "        batch_size = v_k.size(0)\n",
    "        \n",
    "        # get the norm of v_k\n",
    "        v_k_norm = torch.sqrt((v_k ** 2).sum(dim=2, keepdim=True))\n",
    "        \n",
    "        # 0.9 is given for m+ and 0.1 is given for m- from the Dynamic Routing paper \n",
    "        left = F.relu(0.9 - v_k_norm).view(batch_size, -1)\n",
    "        right = F.relu(v_k_norm - 0.1).view(batch_size, -1)\n",
    "        \n",
    "        # 0.5 is given for lamda from the Dynamic Routing paper\n",
    "        loss = T_k * left + 0.5 * (1.0 - T_k) * right\n",
    "        loss = loss.sum(dim=1).mean()\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layer 1 = Convolutional Layer\n",
    "<p><img src = \"./images/capsnet_conv.png\"></p>\n",
    "\n",
    "<p>This is the commonly used convolutional layer present in CNN's. The Input to the Conv Layer is a 28x28 MNIST number. A 256 kernels of size 9 are moved across the image and passed through the ReLU activation function. The output a 256 20x20 images that have been convolved.</p>\n",
    "<p>Input = 28x28 image (one color channel)</p>\n",
    "<p>Number of Kernals = 256</p>\n",
    "<p>Kernal Size = 9x9</p>\n",
    "<p>Stride = 1</p>\n",
    "<p>Output = 20x20x256</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvLayer(nn.Module):\n",
    "    def __init__(self, in_channels=1, out_channels=256, kernel_size=9):\n",
    "        super(ConvLayer, self).__init__()\n",
    "\n",
    "        self.conv = nn.Conv2d(in_channels=in_channels,\n",
    "                              out_channels=out_channels,\n",
    "                              kernel_size=kernel_size,\n",
    "                              stride=1\n",
    "                              )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return F.relu(self.conv(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layer 2 = Primary Caps Layer\n",
    "<p><img src = \"./images/capsnet_PC.png\"></p>\n",
    "<p>This is the first layer of capsules. The capsules primary job is to learn patterns between the features found in the convolution layer before. In the architecture provided by Hinton, this layer has 32 capsules of size 6x6x8. The operation is very similar to a 3d convolutional layer. A kernel of 9X9X256 is moves across the capsules, but instead of max pooling like a traditional CNN, the squash activation is applied.</p>\n",
    "<p>Input = 20x20x256 tensor</p>\n",
    "<p> Number of Capsules = 32</p>\n",
    "<p>Kernal Size = 9x9x256</p>\n",
    "<p>Stride = 2</p>\n",
    "<p>Output = 6x6x8x32 tensor</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrimaryCaps(nn.Module):\n",
    "    def __init__(self, num_capsules=8, in_channels=256, out_channels=32, kernel_size=9, num_routes=32 * 6 * 6):\n",
    "        super(PrimaryCaps, self).__init__()\n",
    "        self.num_routes = num_routes\n",
    "        self.capsules = nn.ModuleList([\n",
    "            nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, stride=2, padding=0)\n",
    "            for _ in range(num_capsules)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        u = [capsule(x) for capsule in self.capsules]\n",
    "        u = torch.stack(u, dim=1)\n",
    "        u = u.view(x.size(0), self.num_routes, -1)\n",
    "        return squash(u)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layer 3 = Digit Caps Layer\n",
    "<p><img src = \"./images/capsnet_DC.png\"></p>\n",
    "<p>This is the second layer of capsules. Each row in the DigitCaps layer represents a capsule for the current class. In our case, on capsule for every number 0-9. The DigitCaps layer receives the 6x6x8x32 output from the PrimaryCaps layer.</p>\n",
    "<p>Input = 6x6x8x32 tensor</p>\n",
    "<p>Number of Capsules = 10 </p>\n",
    "\n",
    "<p>Output = 16x10 matrix</p>\n",
    "<p>The following routing algorithm is provided by Hinton in his paper</p>\n",
    "<p><img src = \"./images/algo.JPG\"></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DigitCaps(nn.Module):\n",
    "    def __init__(self, num_capsules=10, num_routes=32 * 6 * 6, in_channels=8, out_channels=16):\n",
    "        super(DigitCaps, self).__init__()\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.num_routes = num_routes\n",
    "        self.num_capsules = num_capsules\n",
    "\n",
    "        self.W = nn.Parameter(torch.randn(1, num_routes, num_capsules, out_channels, in_channels))\n",
    "\n",
    "    def forward(self, u_i):\n",
    "        ### Routing algorithm\n",
    "        batch_size = u_i.size(0)\n",
    "        \n",
    "        ### Calculate u_hat in order to calculate s_j\n",
    "        ### u_i is the output of the capsules\n",
    "        ### W is the weight matrix\n",
    "        ### u_hat is weight matrix mutliplied by the output u_i\n",
    "        u_i = torch.stack([u_i] * self.num_capsules, dim=2).unsqueeze(4)\n",
    "        W = torch.cat([self.W] * batch_size, dim=0)\n",
    "        u_hat = torch.matmul(W, u_i)\n",
    "\n",
    "        b_ij = Variable(torch.zeros(1, self.num_routes, self.num_capsules, 1))\n",
    "        if USE_CUDA:\n",
    "            b_ij = b_ij.cuda()\n",
    "\n",
    "        ### Loop through number of routing interations\n",
    "        routing_iterations = 3\n",
    "        for iteration in range(routing_iterations): ### line 3\n",
    "            c_ij = F.softmax(b_ij, dim=1)\n",
    "            c_ij = torch.cat([c_ij] * batch_size, dim=0).unsqueeze(4) ### line 4\n",
    "\n",
    "            s_j = (c_ij * u_hat).sum(dim=1, keepdim=True)  ### line 5\n",
    "            v_j = squash(s_j)  ### line 6\n",
    "\n",
    "            if iteration < routing_iterations - 1:\n",
    "                a_ij = torch.matmul(u_hat.transpose(3, 4), torch.cat([v_j] * self.num_routes, dim=1))\n",
    "                b_ij = b_ij + a_ij.squeeze(4).mean(dim=0, keepdim=True)   ### line 7\n",
    "\n",
    "        return v_j.squeeze(1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder\n",
    "<p>Input =&nbsp;16-dimensional vector from the Digit Capsule Layer</p>\n",
    "<p>Output = 28x28 reconstructed image</p>\n",
    "<p>Loss Function =&nbsp;&nbsp;Euclidean distance between the reconstructed image and the input image&nbsp;</p>\n",
    "\n",
    "### Layer 4 = Fully Connected 1\n",
    "<p>Input =&nbsp;16x10 flattened (160 nodes)</p>\n",
    "<p>Output = 512 nodes</p>\n",
    "\n",
    "### Layer 5 = Fully Connected 2\n",
    "<p>Input 512 nodes</p>\n",
    "<p>Output = 1024 nodes</p>\n",
    "\n",
    "### Layer 6 = Fully Connected 3\n",
    "<p>Input 1024 nodes</p>\n",
    "<p>Output = 784 nodes (28x28)</p>\n",
    "\n",
    "<p><img src = \"./images/decoder.png\"></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, input_width=28, input_height=28, input_channel=1):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.input_width = input_width\n",
    "        self.input_height = input_height\n",
    "        self.input_channel = input_channel\n",
    "        self.reconstraction_layers = nn.Sequential(\n",
    "            nn.Linear(16 * 10, 512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(512, 1024),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(1024, self.input_height * self.input_height * self.input_channel),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x, data):\n",
    "        classes = torch.sqrt((x ** 2).sum(2))\n",
    "        classes = F.softmax(classes, dim=0)\n",
    "\n",
    "        _, max_length_indices = classes.max(dim=1)\n",
    "        masked = Variable(torch.sparse.torch.eye(10))\n",
    "        if USE_CUDA:\n",
    "            masked = masked.cuda()\n",
    "        masked = masked.index_select(dim=0, index=Variable(max_length_indices.squeeze(1).data))\n",
    "        t = (x * masked[:, :, None, None]).view(x.size(0), -1)\n",
    "        reconstructions = self.reconstraction_layers(t)\n",
    "        reconstructions = reconstructions.view(-1, self.input_channel, self.input_width, self.input_height)\n",
    "        return reconstructions, masked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "172"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construct Entire Capsule Network Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CapsNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CapsNet, self).__init__()\n",
    "        self.conv_layer = ConvLayer()\n",
    "        self.primary_capsules = PrimaryCaps()\n",
    "        self.digit_capsules = DigitCaps()\n",
    "        self.decoder = Decoder()\n",
    "        self.mse_loss = nn.MSELoss()\n",
    "\n",
    "    def forward(self, data):\n",
    "        output = self.digit_capsules(self.primary_capsules(self.conv_layer(data)))\n",
    "        reconstructions, masked = self.decoder(output, data)\n",
    "        return output, reconstructions, masked\n",
    "\n",
    "    def loss(self, data, x, target, reconstructions):\n",
    "        return margin_loss(x, target) + self.reconstruction_loss(data, reconstructions)\n",
    "\n",
    "\n",
    "    def reconstruction_loss(self, data, reconstructions):\n",
    "        loss = self.mse_loss(reconstructions.view(reconstructions.size(0), -1), data.view(reconstructions.size(0), -1))\n",
    "        return loss * 0.0005"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and Test Loops for the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, train_loader, epoch):\n",
    "    capsule_net = model\n",
    "    capsule_net.train()\n",
    "    n_batch = len(list(enumerate(train_loader)))\n",
    "    total_loss = 0\n",
    "    for batch_id, (data, target) in enumerate(tqdm(train_loader)):\n",
    "\n",
    "        target = torch.sparse.torch.eye(10).index_select(dim=0, index=target)\n",
    "        data, target = Variable(data), Variable(target)\n",
    "\n",
    "        if USE_CUDA:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output, reconstructions, masked = capsule_net(data)\n",
    "        loss = capsule_net.loss(data, output, target, reconstructions)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        correct = sum(np.argmax(masked.data.cpu().numpy(), 1) == np.argmax(target.data.cpu().numpy(), 1))\n",
    "        train_loss = loss.data\n",
    "        total_loss += train_loss\n",
    "        if batch_id % 100 == 0:\n",
    "            tqdm.write(\"Epoch: [{}/{}], Batch: [{}/{}], train accuracy: {:.6f}, loss: {:.6f}\".format(\n",
    "                epoch,\n",
    "                N_EPOCHS,\n",
    "                batch_id + 1,\n",
    "                n_batch,\n",
    "                correct / float(BATCH_SIZE),\n",
    "                train_loss / float(BATCH_SIZE)\n",
    "                ))\n",
    "    tqdm.write('Epoch: [{}/{}], train loss: {:.6f}'.format(epoch,N_EPOCHS,total_loss / len(train_loader.dataset)))\n",
    "    \n",
    "def test(capsule_net, test_loader, epoch, name):\n",
    "    capsule_net.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    for batch_id, (data, target) in enumerate(test_loader):\n",
    "\n",
    "        target = torch.sparse.torch.eye(10).index_select(dim=0, index=target)\n",
    "        data, target = Variable(data), Variable(target)\n",
    "\n",
    "        if USE_CUDA:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "\n",
    "        output, reconstructions, masked = capsule_net(data)\n",
    "        loss = capsule_net.loss(data, output, target, reconstructions)\n",
    "\n",
    "        test_loss += loss.data\n",
    "        correct += sum(np.argmax(masked.data.cpu().numpy(), 1) ==\n",
    "                       np.argmax(target.data.cpu().numpy(), 1))\n",
    "\n",
    "    tqdm.write(\n",
    "        \"Epoch: [{}/{}], test accuracy {}: {:.6f}, loss: {:.6f}\".format(epoch, N_EPOCHS, str(name), correct / len(test_loader.dataset),\n",
    "                                                                  test_loss / len(test_loader)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run the Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-a6fc359ba822>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN_EPOCHS\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcapsule_net\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcapsule_net\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"NORMAL TEST\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcapsule_net\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader_rotate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ROTATED TEST\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-27-7790f88dcf66>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, optimizer, train_loader, epoch)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mcapsule_net\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mcapsule_net\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mn_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mtotal_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    613\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_workers\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# same-process loading\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    614\u001b[0m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_iter\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 615\u001b[0;31m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    616\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    617\u001b[0m                 \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpin_memory_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    613\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_workers\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# same-process loading\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    614\u001b[0m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_iter\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 615\u001b[0;31m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    616\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    617\u001b[0m                 \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpin_memory_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/torchvision/datasets/mnist.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0;31m# doing this so that it is consistent with all other datasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0;31m# to return a PIL Image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfromarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'L'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/PIL/Image.py\u001b[0m in \u001b[0;36mfromarray\u001b[0;34m(obj, mode)\u001b[0m\n\u001b[1;32m   2506\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mversionadded\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m1.1\u001b[0m\u001b[0;36m.6\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2507\u001b[0m     \"\"\"\n\u001b[0;32m-> 2508\u001b[0;31m     \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__array_interface__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2509\u001b[0m     \u001b[0mshape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'shape'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2510\u001b[0m     \u001b[0mndim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "gc.collect()\n",
    "capsule_net = CapsNet()\n",
    "\n",
    "if USE_CUDA:\n",
    "    capsule_net = capsule_net.cuda()\n",
    "\n",
    "optimizer = torch.optim.Adam(capsule_net.parameters())\n",
    "\n",
    "for e in range(1, N_EPOCHS + 1):\n",
    "    train(capsule_net, optimizer, train_loader, e)\n",
    "    test(capsule_net, test_loader, e, \"NORMAL TEST\")\n",
    "    test(capsule_net, test_loader_rotate, e, \"ROTATED TEST\")\n",
    "    test(capsule_net, test_loader_scale_half, e, \"SCALE TEST\")\n",
    "\n",
    "\n",
    "torch.save(capsule_net.state_dict(), './capsnet_trained.model')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Sources</h2>\n",
    "<p>1.&nbsp;<a href=\"https://arxiv.org/pdf/1710.09829.pdf\">Dynamic Routing Between Capsules</a>&nbsp;</p>\n",
    "<p>2. <a href=\"https://openreview.net/pdf?id=HJWLfGWRb\">Matrix Capsules with EM Routing</a></p>\n",
    "<p>3.&nbsp;<a href=\"http://www.cs.toronto.edu/~fritz/absps/transauto6.pdf\">Transforming Auto-encoders</a></p>\n",
    "<p>&nbsp;</p>\n",
    "<p>&nbsp;</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
