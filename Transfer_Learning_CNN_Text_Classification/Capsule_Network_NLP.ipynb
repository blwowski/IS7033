{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Capsule Network on the classic IMDB Sentiment</h2>\n",
    "<p>The goal of this notebook is to explore the Capsule Network on the classic IMDB Sentiment Analysis dataset. I would like to investigate how well the CapsNet does on NLP task such as sentiment analysis and compare it to the state of the art.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Import Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import gc\n",
    "import re\n",
    "\n",
    "import gensim\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data_utils\n",
    "\n",
    "\n",
    "from torchtext import data\n",
    "from torchtext import datasets\n",
    "from torchtext.vocab import Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Data and Prepare it for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data/imdb_master.csv', encoding=\"latin-1\")\n",
    "df = df.drop(['Unnamed: 0', 'file'], axis=1)\n",
    "\n",
    "df = df[df['label'] != 'unsup']\n",
    "df['label'] = df['label'].replace('neg', 0)\n",
    "df['label'] = df['label'].replace('pos', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['label'] = df['label'].replace('neg', 0)\n",
    "df['label'] = df['label'].replace('pos', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Contractions Dictionary\n",
    "contractions = { \n",
    "\"ain't\": \"am not\",\n",
    "\"aren't\": \"are not\",\n",
    "\"can't\": \"cannot\",\n",
    "\"can't've\": \"cannot have\",\n",
    "\"'cause\": \"because\",\n",
    "\"could've\": \"could have\",\n",
    "\"couldn't\": \"could not\",\n",
    "\"couldn't've\": \"could not have\",\n",
    "\"didn't\": \"did not\",\n",
    "\"doesn't\": \"does not\",\n",
    "\"don't\": \"do not\",\n",
    "\"hadn't\": \"had not\",\n",
    "\"hadn't've\": \"had not have\",\n",
    "\"hasn't\": \"has not\",\n",
    "\"haven't\": \"have not\",\n",
    "\"he'd\": \"he had\",\n",
    "\"he'd've\": \"he would have\",\n",
    "\"he'll\": \"he will\",\n",
    "\"he'll've\": \"he will have\",\n",
    "\"he's\": \" he is\",\n",
    "\"how'd\": \"how did\",\n",
    "\"how'd'y\": \"how do you\",\n",
    "\"how'll\": \"how will\",\n",
    "\"how's\": \"how is\",\n",
    "\"I'd\": \"I had\",\n",
    "\"I'd've\": \"I would have\",\n",
    "\"I'll\": \"I will\",\n",
    "\"i'll\": \"I will\",\n",
    "\"I'll've\": \"I will have\",\n",
    "\"I'm\": \"I am\",\n",
    "\"I've\": \"I have\",\n",
    "\"isn't\": \"is not\",\n",
    "\"it'd\": \"it would\",\n",
    "\"it'd've\": \"it would have\",\n",
    "\"it'll\": \"it will\",\n",
    "\"it'll've\": \"it will have\",\n",
    "\"it's\": \"it is\",\n",
    "\"it's\": \"it is\",\n",
    "\"let's\": \"let us\",\n",
    "\"ma'am\": \"madam\",\n",
    "\"mayn't\": \"may not\",\n",
    "\"might've\": \"might have\",\n",
    "\"mightn't\": \"might not\",\n",
    "\"mightn't've\": \"might not have\",\n",
    "\"must've\": \"must have\",\n",
    "\"mustn't\": \"must not\",\n",
    "\"mustn't've\": \"must not have\",\n",
    "\"needn't\": \"need not\",\n",
    "\"needn't've\": \"need not have\",\n",
    "\"o'clock\": \"of the clock\",\n",
    "\"oughtn't\": \"ought not\",\n",
    "\"oughtn't've\": \"ought not have\",\n",
    "\"shan't\": \"shall not\",\n",
    "\"sha'n't\": \"shall not\",\n",
    "\"shan't've\": \"shall not have\",\n",
    "\"she'd\": \"she had\",\n",
    "\"she'd've\": \"she would have\",\n",
    "\"she'll\": \"she will\",\n",
    "\"she'll've\": \"she will have\",\n",
    "\"she's\": \"she is\",\n",
    "\"should've\": \"should have\",\n",
    "\"shouldn't\": \"should not\",\n",
    "\"shouldn't've\": \"should not have\",\n",
    "\"so've\": \"so have\",\n",
    "\"so's\": \"so is\",\n",
    "\"that'd\": \"that had\",\n",
    "\"that'd've\": \"that would have\",\n",
    "\"that's\": \"that is\",\n",
    "\"there'd\": \"there would\",\n",
    "\"there'd've\": \"there would have\",\n",
    "\"there's\": \"there is\",\n",
    "\"they'd\": \"they had / they would\",\n",
    "\"they'd've\": \"they would have\",\n",
    "\"they'll\": \"they will\",\n",
    "\"they'll've\": \"they will have\",\n",
    "\"they're\": \"they are\",\n",
    "\"they've\": \"they have\",\n",
    "\"to've\": \"to have\",\n",
    "\"wasn't\": \"was not\",\n",
    "\"we'd\": \"we would\",\n",
    "\"we'd've\": \"we would have\",\n",
    "\"we'll\": \"we will\",\n",
    "\"we'll've\": \"we will have\",\n",
    "\"We're\": \"we are\",\n",
    "\"we're\": \"we are\",\n",
    "\"we've\": \"we have\",\n",
    "\"weren't\": \"were not\",\n",
    "\"what'll\": \"what will\",\n",
    "\"what'll've\": \"what will have\",\n",
    "\"what're\": \"what are\",\n",
    "\"what's\": \"what is\",\n",
    "\"what've\": \"what have\",\n",
    "\"when's\": \"when is\",\n",
    "\"when've\": \"when have\",\n",
    "\"where'd\": \"where did\",\n",
    "\"where's\": \"where is\",\n",
    "\"where've\": \"where have\",\n",
    "\"who'll\": \"who will\",\n",
    "\"who'll've\": \"who will have\",\n",
    "\"who's\": \"who has\",\n",
    "\"who've\": \"who have\",\n",
    "\"why's\": \"why is\",\n",
    "\"why've\": \"why have\",\n",
    "\"will've\": \"will have\",\n",
    "\"won't\": \"will not\",\n",
    "\"won't've\": \"will not have\",\n",
    "\"would've\": \"would have\",\n",
    "\"wouldn't\": \"would not\",\n",
    "\"wouldn't've\": \"would not have\",\n",
    "\"y'all\": \"you all\",\n",
    "\"y'all'd\": \"you all would\",\n",
    "\"y'all'd've\": \"you all would have\",\n",
    "\"y'all're\": \"you all are\",\n",
    "\"y'all've\": \"you all have\",\n",
    "\"you'd\": \"you would\",\n",
    "\"you'd've\": \"you would have\",\n",
    "\"you'll\": \"you will\",\n",
    "\"you'll've\": \"you will have\",\n",
    "\"you're\": \"you are\",\n",
    "\"you've\": \"you have\"\n",
    "}\n",
    "\n",
    "contractions_re = re.compile('(%s)' % '|'.join(contractions.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "### function to clean the comment_text data\n",
    "def expand_contractions(s, contractions_dict=contractions):\n",
    "    def replace(match):\n",
    "        return contractions_dict[match.group(0)]\n",
    "    return contractions_re.sub(replace, s)\n",
    "\n",
    "\n",
    "def clean_data(text):\n",
    "    text = expand_contractions(text, contractions)\n",
    "    text = str(text).lower()\n",
    "    text = re.sub(r'\\w+:\\/{2}[\\d\\w-]+(\\.[\\d\\w-]+)*(?:(?:\\/[^\\s/]*))*', ' url ', text)\n",
    "    text = re.sub('(!+)', '!', text)\n",
    "    text = re.sub('(\\?+)', '?', text)\n",
    "    text = re.sub('(\\s+)', ' ', text)\n",
    "    text = re.sub('(\\\"+)', ' \\\" ', text)\n",
    "    text = re.sub('(\\.+)', '\\.', text)\n",
    "    text = re.sub('(<+)', ' < .', text)\n",
    "    text = re.sub('(>+)', ' > .', text)\n",
    "    text = text.replace(\"\\\\\", \" \")\n",
    "    text = text.replace(\"-\", \" \")\n",
    "    text = text.replace(\"â€”\", \" \")\n",
    "    text = text.replace(\"/\", \" \")\n",
    "    text = text.replace(\"_\", \" \")\n",
    "    text = re.sub('([.,!?()])', r' \\1 ', text)\n",
    "    text = re.sub('\\s{2,}', ' ', text)\n",
    "    text = re.sub('[^A-Za-z0-9\\?\\!\\.<>,\\s]+', '', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_pad_slice_column_to_int(df, row_name, vocab_dict, max_sequence_length):\n",
    "    processed_row = []\n",
    "    \n",
    "    for i, row in df.iterrows():\n",
    "        text = clean_data(str(row[row_name]))\n",
    "        text_arr = text.split(\" \")\n",
    "        \n",
    "        if len(text_arr) > max_sequence_length:\n",
    "            text_arr = text_arr[:max_sequence_length]\n",
    "        elif len(text_arr) < max_sequence_length:\n",
    "            pad_to_add = max_sequence_length-len(text_arr)\n",
    "            text_arr.extend(['<pad>' for i in range(pad_to_add)])\n",
    "        \n",
    "        for i in range(len(text_arr)):\n",
    "            if text_arr[i] in vocab_dict:\n",
    "                text_arr[i] = vocab_dict[text_arr[i]]\n",
    "            else:\n",
    "                text_arr[i] = vocab_dict[text_arr[1]]\n",
    "            \n",
    "        processed_row.append(np.array(text_arr))\n",
    "        \n",
    "    df[\"processed_row\"] = processed_row\n",
    "    return df\n",
    "\n",
    "def create_vocab_from_df_column(df, row_name):\n",
    "    vocab_dict = {'<pad>' : 0,\n",
    "                  '<unk>' : 1}\n",
    "    index = 2\n",
    "    \n",
    "    for i, row in df.iterrows():\n",
    "        text = clean_data(str(row[row_name]))\n",
    "        text_arr = text.split(\" \")\n",
    "        for word in text_arr:\n",
    "            if word in vocab_dict:\n",
    "                continue\n",
    "            else:\n",
    "                vocab_dict[word] = index\n",
    "                index += 1\n",
    "                \n",
    "    return vocab_dict\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_dict = create_vocab_from_df_column(df, 'review')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = tokenize_pad_slice_column_to_int(df, 'review', vocab_dict, 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>review</th>\n",
       "      <th>label</th>\n",
       "      <th>processed_row</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>test</td>\n",
       "      <td>Once again Mr. Costner has dragged out a movie...</td>\n",
       "      <td>0</td>\n",
       "      <td>[2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>test</td>\n",
       "      <td>This is an example of why the majority of acti...</td>\n",
       "      <td>0</td>\n",
       "      <td>[98, 61, 120, 123, 25, 104, 19, 124, 25, 125, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>test</td>\n",
       "      <td>First of all I hate those moronic rappers, who...</td>\n",
       "      <td>0</td>\n",
       "      <td>[236, 25, 101, 31, 237, 238, 239, 240, 24, 72,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>test</td>\n",
       "      <td>Not even the Beatles could write songs everyon...</td>\n",
       "      <td>0</td>\n",
       "      <td>[34, 324, 19, 325, 116, 326, 327, 328, 329, 24...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>test</td>\n",
       "      <td>Brass pictures (movies is not a fitting word f...</td>\n",
       "      <td>0</td>\n",
       "      <td>[441, 442, 122, 336, 61, 34, 10, 443, 444, 12,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   type                                             review  label  \\\n",
       "0  test  Once again Mr. Costner has dragged out a movie...      0   \n",
       "1  test  This is an example of why the majority of acti...      0   \n",
       "2  test  First of all I hate those moronic rappers, who...      0   \n",
       "3  test  Not even the Beatles could write songs everyon...      0   \n",
       "4  test  Brass pictures (movies is not a fitting word f...      0   \n",
       "\n",
       "                                       processed_row  \n",
       "0  [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 1...  \n",
       "1  [98, 61, 120, 123, 25, 104, 19, 124, 25, 125, ...  \n",
       "2  [236, 25, 101, 31, 237, 238, 239, 240, 24, 72,...  \n",
       "3  [34, 324, 19, 325, 116, 326, 327, 328, 329, 24...  \n",
       "4  [441, 442, 122, 336, 61, 34, 10, 443, 444, 12,...  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create batch generator\n",
    "df_train = df[df['type'] == 'train'].sample(frac=1).reset_index(drop=True).drop(['type'], axis = 1)\n",
    "df_train.label = df_train.label.astype('float64')\n",
    "\n",
    "df_test = df[df['type'] == 'test'].sample(frac=1).reset_index(drop=True).drop(['type'], axis = 1)\n",
    "df_test.label = df_test.label.astype('float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Input Shape: torch.Size([25000, 300]) Train Target Shape: torch.Size([25000, 1])\n",
      "Test Input Shape: torch.Size([25000, 300]) Test Target Shape: torch.Size([25000, 1])\n"
     ]
    }
   ],
   "source": [
    "X_train = df_train.processed_row.tolist()\n",
    "X_train = torch.from_numpy(np.vstack(X_train))\n",
    "\n",
    "X_test = df_test.processed_row.tolist()\n",
    "X_test = torch.from_numpy(np.vstack(X_test))\n",
    "\n",
    "y_train = df_train.label.tolist()\n",
    "#y_train = torch.from_numpy(np.vstack(y_train), dtype=torch.long)\n",
    "y_train = torch.tensor(np.vstack(y_train), dtype=torch.float)\n",
    "\n",
    "y_test = df_test.label.to_list()\n",
    "#y_test = torch.from_numpy(np.vstack(y_test))\n",
    "y_test = torch.tensor(np.vstack(y_train), dtype=torch.float)\n",
    "\n",
    "print(\"Train Input Shape: {} Train Target Shape: {}\".format(X_train.shape, y_train.shape))\n",
    "print(\"Test Input Shape: {} Test Target Shape: {}\".format(X_test.shape, y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = data_utils.TensorDataset(X_train, y_train)\n",
    "train_loader = data_utils.DataLoader(train, batch_size=50, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = data_utils.TensorDataset(X_test, y_test)\n",
    "test_loader = data_utils.DataLoader(train, batch_size=50, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_wv_matrix(vocab_dict):\n",
    "    print ('... Loading Word Vectors')\n",
    "    word_vectors = KeyedVectors.load_word2vec_format(\"./models/GoogleNews-vectors-negative300.bin\", binary=True)\n",
    "    wv_matrix = []\n",
    "    count = 0\n",
    "    print ('... Finish Loading Word Vectors')\n",
    "    \n",
    "    for each in vocab_dict.items():\n",
    "        count += 1\n",
    "        \n",
    "        word = str(each[0]).lower()\n",
    "        index = int(each[1])\n",
    "        \n",
    "        if word in word_vectors.vocab:\n",
    "            wv_matrix.append(word_vectors.word_vec(word))\n",
    "        else:\n",
    "            wv_matrix.append(np.random.uniform(-0.01, 0.01, 300).astype(\"float32\"))\n",
    "        \n",
    "        if count %10000 == 0:\n",
    "            print (\"On Index {}\".format(count))\n",
    "            \n",
    "    ### Add Unknown Token\n",
    "    wv_matrix.append(np.random.uniform(-0.01, 0.01, 300).astype(\"float32\"))\n",
    "    ### Add Pad Token\n",
    "    wv_matrix.append(np.zeros(300).astype(\"float32\"))\n",
    "    print ('... Finished Creating Matrix')\n",
    "    \n",
    "    del(word_vectors)\n",
    "    \n",
    "    return np.array(wv_matrix)\n",
    "\n",
    "def create_emb_layer(weights_matrix, non_trainable=False):\n",
    "    num_embeddings, embedding_dim = weights_matrix.shape\n",
    "    emb_layer = nn.Embedding(num_embeddings, embedding_dim)\n",
    "    emb_layer.weight.data.copy_(torch.from_numpy(weights_matrix))\n",
    "    \n",
    "    if non_trainable:\n",
    "        emb_layer.weight.requires_grad = False\n",
    "\n",
    "    return emb_layer, num_embeddings, embedding_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... Loading Word Vectors\n",
      "... Finish Loading Word Vectors\n",
      "On Index 10000\n",
      "On Index 20000\n",
      "On Index 30000\n",
      "On Index 40000\n",
      "On Index 50000\n",
      "On Index 60000\n",
      "On Index 70000\n",
      "On Index 80000\n",
      "On Index 90000\n",
      "On Index 100000\n",
      "... Finished Creating Matrix\n"
     ]
    }
   ],
   "source": [
    "### Get Word Vector Matrix\n",
    "wv_matrix = create_wv_matrix(vocab_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50, 300, 300])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Test the Embedding Layer\n",
    "emb_layer, num_embeddings, embedding_dim = create_emb_layer(wv_matrix)\n",
    "batch = next(iter(train_loader))\n",
    "emb_layer(batch[0]).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, weights_matrix):\n",
    "        super(CNN, self).__init__()\n",
    "        \n",
    "        ### Embedding Layer\n",
    "        self.embedding, num_embeddings, embedding_dim = create_emb_layer(weights_matrix, True)\n",
    "       \n",
    "        ### Convolution Layer 1\n",
    "        self.conv1 = nn.Sequential(         # input shape (1, 300, 300)\n",
    "            nn.Conv2d(\n",
    "                in_channels=1,              # input height\n",
    "                out_channels=3,             # n_filters\n",
    "                kernel_size=10,             # filter size\n",
    "                stride=2,                   # filter movement/step\n",
    "                padding=0,                  # if want same width and length of this image after Conv2d, padding=(kernel_size-1)/2 if stride=1\n",
    "            ),                              # output shape (3, 146, 146)\n",
    "            nn.ReLU(),                      # activation\n",
    "            nn.MaxPool2d(kernel_size=2),    # (146-2 / 2) choose max value in 2x2 area, output shape (3, 73, 73)\n",
    "        )\n",
    "        \n",
    "        ### Convolution Layer 2\n",
    "        self.conv2 = nn.Sequential(        # input shape (3, 73, 73)\n",
    "            nn.Conv2d(3, 3, 5, 2, 1),      # output shape (3, 36, 36)\n",
    "            nn.ReLU(),                     # activation\n",
    "            nn.MaxPool2d(2),               # output shape (3, 18, 18)\n",
    "        )\n",
    "            \n",
    "        ### Fully Connected Layer 3\n",
    "        self.FC1 = nn.Linear(3 * 18 * 18, 800)\n",
    "        \n",
    "        ### Fully Connected Layer 4\n",
    "        self.FC2 = nn.Linear(800, 200)\n",
    "        \n",
    "        # Output 2 classes\n",
    "        self.out = nn.Linear(200, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        #print (x.shape)\n",
    "        x = x.unsqueeze(1)\n",
    "        #print (x.shape)\n",
    "        x = self.conv1(x)\n",
    "        #print (x.shape)\n",
    "        x = self.conv2(x)\n",
    "        #print (x.shape)\n",
    "        x = x.view(x.size(0), -1)           # flatten the output of conv2\n",
    "        x = F.relu(self.FC1(x))\n",
    "        #print (x.shape)\n",
    "        x = F.relu(self.FC2(x))\n",
    "        #print (x.shape)\n",
    "        output =  torch.sigmoid(self.out(x))\n",
    "        #print (output.shape)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Hyperparameters\n",
    "num_epochs = 250\n",
    "cnn_classifier = CNN(weights_matrix=wv_matrix)\n",
    "\n",
    "optimizer = optim.Adam(cnn_classifier.parameters())\n",
    "criterion = nn.MSELoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_accuracy(preds, y):\n",
    "    #round predictions to the closest integer\n",
    "    rounded_preds = torch.round(preds)\n",
    "    correct = (rounded_preds == y).float() #convert into float for division \n",
    "    acc = correct.sum() / len(correct)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    count = 1\n",
    "    model.train()\n",
    "    \n",
    "    for batch in iterator:\n",
    "        count +=1\n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(batch[0])\n",
    "        loss = criterion(predictions, batch[1])\n",
    "        acc = binary_accuracy(predictions, batch[1])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in iterator:\n",
    "            predictions = model(batch[0])\n",
    "            loss = criterion(predictions, batch[1])\n",
    "            acc = binary_accuracy(predictions, batch[1])\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 0, TIME: 1:30\n",
      "Train Loss: 0.24940410068631172. Train Acc: 49.9999998986721\n",
      "Test Loss: 0.24944507920742034. Test Acc: 49.99999991655349\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-56-ea8b9d8aaad3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcnn_classifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mtest_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcnn_classifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-41-7b3226d899d4>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, iterator, optimizer, criterion)\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbinary_accuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \"\"\"\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "### Training Loop\n",
    "best_test_loss = float('inf')\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss, train_acc = train(cnn_classifier, train_loader, optimizer, criterion)\n",
    "    test_loss, test_acc = evaluate(cnn_classifier, test_loader, criterion)\n",
    "    \n",
    "    end_time = time.time()\n",
    "\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    if test_loss < best_test_loss:\n",
    "        best_test_loss = test_loss\n",
    "        torch.save(cnn_classifier.state_dict(), 'tut4-model.pt')\n",
    "    \n",
    "    print(\"EPOCH: {}, TIME: {}:{}\".format(epoch, epoch_mins, epoch_secs))\n",
    "    print(\"Train Loss: {}. Train Acc: {}\".format(train_loss, train_acc*100))\n",
    "    print(\"Test Loss: {}. Test Acc: {}\".format(test_loss, test_acc*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
